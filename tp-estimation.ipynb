{"metadata":{"kernelspec":{"display_name":"Python 3.8.11 64-bit ('sdia-estimation': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"interpreter":{"hash":"5c7a02607861a5057e1d62a2006e259fdb8ddc0867b84f038715ad8928cc603b"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c32403e8","cell_type":"markdown","source":"# TP ESTIMATION\n\n**Elève 1** : Mohamed Ben ALI\n","metadata":{}},{"id":"45cf7ae0","cell_type":"markdown","source":"## Import des fonctions","metadata":{}},{"id":"4d476b64","cell_type":"code","source":"# Importing libraries\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.stats import uniform\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:33:09.545800Z","iopub.execute_input":"2024-11-30T15:33:09.546246Z","iopub.status.idle":"2024-11-30T15:33:10.572877Z","shell.execute_reply.started":"2024-11-30T15:33:09.546212Z","shell.execute_reply":"2024-11-30T15:33:10.571883Z"}},"outputs":[],"execution_count":11},{"id":"093fcc27","cell_type":"code","source":"# Probability density of a Gaussian mixture at a point\ndef gm_pdf(x, mu, sigma, p):\n    # Initialization of the output variable\n    result = 0.0\n    # Check the consistency of the input parameters\n    # The mean vector must have the same length as the probability vector p\n    if len(mu) != len(p):\n        print('Dimension error in the mean vector')\n    # The standard deviation vector must have the same length as the probability vector p\n    elif len(sigma) != len(p):\n        print('Dimension error in the standard deviation vector')\n    else:\n        # Calculation of the density value\n        for i in range(0, len(p)):\n            result = result + p[i] * norm.pdf(x, mu[i], sigma[i])\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:33:18.979418Z","iopub.execute_input":"2024-11-30T15:33:18.979964Z","iopub.status.idle":"2024-11-30T15:33:18.986514Z","shell.execute_reply.started":"2024-11-30T15:33:18.979927Z","shell.execute_reply":"2024-11-30T15:33:18.985446Z"}},"outputs":[],"execution_count":12},{"id":"959a0aa5","cell_type":"code","source":"# Generation of random numbers following a mixture of Gaussians\ndef gm_rnd(mu, sigma, p):\n    # Initialization of the output variable\n    result = 0.0\n    # Check the consistency of the input parameters\n    # The mean vector must have the same length as the probability vector p\n    if len(mu) != len(p):\n        print('Dimension error in the mean vector')\n    # The standard deviation vector must have the same length as the probability vector p\n    elif len(sigma) != len(p):\n        print('Dimension error in the standard deviation vector')\n    else:\n        # Sample generation\n        # A sample is drawn from a uniform distribution on [0,1]\n        u = uniform.rvs(loc=0.0, scale=1.0, size=1)\n        # Each subsequent test defines an interval where the probability\n        # of the uniform variable belonging to it matches one of the probabilities\n        # defined in the probability vector p. If u belongs to one of these intervals,\n        # it is equivalent to having generated a random variable according to one of the\n        # elements of p. For example, in the first test below, the probability of u\n        # being in the interval [0, p[0]] is equal to p[0] since u follows a uniform\n        # distribution. Therefore, if u belongs to [0, p[0]], it is equivalent to\n        # drawing according to the event with probability p[0].\n        if u < p[0]:  # Check if an event with probability p[0] was generated\n            result = sigma[0] * norm.rvs(loc=0, scale=1, size=1) + mu[0]\n            # To generate a random variable following any normal distribution, simply\n            # multiply a standard normal variable (mean 0 and standard deviation 1)\n            # by the desired standard deviation and add the desired mean to the result.\n        for i in range(1, len(p)):\n            if (u > np.sum(p[0:i])) and (u <= np.sum(p[0:i+1])):  # Check if an event\n                # with probability p[i] was generated\n                result = sigma[i] * norm.rvs(loc=0.0, scale=1.0, size=1) + mu[i]\n                # To generate a random variable following any normal distribution, simply\n                # multiply a standard normal variable (mean 0 and standard deviation 1)\n                # by the desired standard deviation and add the desired mean to the result.\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:33:20.439496Z","iopub.execute_input":"2024-11-30T15:33:20.440499Z","iopub.status.idle":"2024-11-30T15:33:20.450052Z","shell.execute_reply.started":"2024-11-30T15:33:20.440447Z","shell.execute_reply":"2024-11-30T15:33:20.448929Z"}},"outputs":[],"execution_count":13},{"id":"599686fc","cell_type":"code","source":"def EM_algorithm(nbMaxIterations, mu_0, sigma_0, alpha_0, donnees):\n\n    # Initialization values\n    mu_em = mu_0\n    sigma_em = sigma_0\n    alpha_em = alpha_0  # The sum must equal 1\n\n    nbIteration = 1  # Initialization of the stopping variable\n    nbComposante = len(alpha_em)  # Number of mixture components\n    nbDonnees = len(donnees)  # Number of data points\n    p = np.zeros(shape=(nbComposante, nbDonnees))\n    # Declaration and initialization of the matrix that will hold probabilities\n    # p(k|x, current_theta)\n\n    alpha_em_new = alpha_em\n    sigma_em_carre_new = sigma_em\n    mu_em_new = mu_em\n    donneesP = np.zeros(shape=(nbDonnees))\n\n    while nbIteration < nbMaxIterations:\n        # Expectation step: calculate probabilities\n        for n in range(0, nbDonnees, 1):\n            for k in range(0, nbComposante, 1):\n                p[k, n] = alpha_em[k] * norm.pdf(x=donnees[n], loc=mu_em[k], scale=sigma_em[k])\n            p[:, n] = p[:, n] / np.sum(p[:, n])  # Normalize probabilities\n\n        # Maximization step: update parameters\n        for k in range(0, nbComposante, 1):\n            alpha_em_new[k] = np.sum(p[k, :]) / nbDonnees  # Update weights\n            for n in range(0, nbDonnees, 1):\n                donneesP[n] = donnees[n] * p[k, n]\n            mu_em_new[k] = np.sum(donneesP) / np.sum(p[k, :])  # Update means\n            for n in range(nbDonnees):\n                donneesP[n] = ((donnees[n] - mu_em_new[k]) ** 2) * p[k, n]\n            sigma_em_carre_new[k] = np.sum(donneesP) / np.sum(p[k, :])  # Update variances\n        mu_em = mu_em_new\n        sigma_em = np.sqrt(sigma_em_carre_new)  # Convert variances to standard deviations\n        alpha_em = alpha_em_new\n        nbIteration = nbIteration + 1\n\n    return np.array([mu_em, sigma_em, alpha_em])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:33:55.519400Z","iopub.execute_input":"2024-11-30T15:33:55.519809Z","iopub.status.idle":"2024-11-30T15:33:55.531290Z","shell.execute_reply.started":"2024-11-30T15:33:55.519775Z","shell.execute_reply":"2024-11-30T15:33:55.530226Z"}},"outputs":[],"execution_count":14},{"id":"aa864bd3","cell_type":"markdown","source":"## Part 1: Testing the EM Algorithm on Synthetic Data\nAfter testing the EM algorithm on synthetic data, the following observations were made:\n\nInitial Values Impact: Changing the initial values can significantly influence the estimation. Choosing initial values that are \"close\" to the true values leads to better estimations. Therefore, it is preferable to determine an approximate range for the desired parameters before using the EM algorithm.\n\nEffect of the Number of Iterations: With a good choice of initial parameters, the number of iterations seems to have little effect on the estimation. However, selecting too few iterations can prevent the algorithm from converging properly.","metadata":{}},{"id":"8c2f353a","cell_type":"markdown","source":"## Part 2: Galaxies\nWe will attempt to estimate the distribution of the values contained in galaxies.txt as a Gaussian mixture model. Before we begin, we need to determine the number of Gaussians in the mixture, and for each of them, we must set an initialization for:\n\nIts mean $\\mu_0$\nIts standard deviation $\\sigma_0$\nIts \"probability\" $\\alpha_0$\nTo simplify the visualization of the data and avoid potential effects caused by the large values of velocities, we will center and scale the data. In the two figures below, we will first display the data as a scatter plot, and then as a histogram.\n\n\n\n\n\n\n4o mini","metadata":{}},{"id":"b594397c-64f2-48be-bb6d-0ed2e1089609","cell_type":"code","source":"x_galaxies = np.loadtxt(\"galaxies.txt\")\nx_galaxies = (x_galaxies - np.mean(x_galaxies)) / np.std(x_galaxies) \nnb_samples = len(x_galaxies)\n\nplt.plot(x_galaxies, 'g.')\nplt.title('Distribution des vitesses')\nplt.xlabel('Indice')\nplt.ylabel('Vitesse')\nplt.grid()\nplt.show()\n\ndef plot_galaxy_hist():\n    plt.hist(x_galaxies, bins = 30, density = True, edgecolor = \"black\")\n    plt.title('Distribution des vitesses')\n    plt.xlabel('Vitesse')\n    plt.ylabel(\"Nombre d'échantillons\")\n\nplot_galaxy_hist()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:35:04.470243Z","iopub.execute_input":"2024-11-30T15:35:04.471286Z","iopub.status.idle":"2024-11-30T15:35:04.843620Z","shell.execute_reply.started":"2024-11-30T15:35:04.471245Z","shell.execute_reply":"2024-11-30T15:35:04.841904Z"}},"outputs":[],"execution_count":null},{"id":"3624698b","cell_type":"markdown","source":"## Comment\n\nIn the first step, we will focus on the main trends we can observe in these two plots. Three distinct groups seem to emerge, so we will choose a mixture of three Gaussians. For the initialization values, we calculate the mean and standard deviation of three groups, selected for now \"by eye\" by observing the curves:\n\nBetween the first and the 8th value\nBetween the 8th and the 76th value\nAnd from the 76th value to the last one.","metadata":{}},{"id":"60d346aa","cell_type":"code","source":"# Chose the velues\nbatches = np.split(x_galaxies, [8,76])\nmu_0 = np.array([np.mean(batch) for batch in batches])\nsigma_0 = np.array([np.std(batch) for batch in batches])\nalpha_0 = np.array([0.2, 0.7, 0.1])\nnb_iterations = 40","metadata":{},"outputs":[],"execution_count":null},{"id":"55653946","cell_type":"code","source":"# Execution\nresultat = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\nmu_em, sigma_em, alpha_em = resultat\n\ndef print_parameters(mu, sigma, alpha):\n    print('Les paramètres estimés sont : ')\n    print('Moyennes des composantes du mélange', mu)\n    print('Ecart type des composantes du mélange', sigma)\n    print('Probabilités des composantes du mélange', alpha)\n    \nprint_parameters(mu_em, sigma_em, alpha_em)","metadata":{},"outputs":[],"execution_count":null},{"id":"dab2bf66","cell_type":"code","source":"# Display distribution\n\nx = np.linspace(-3, 3, 10000)\ndef plot_distribution_comparison(estimation):\n    plot_galaxy_hist()\n    plt.plot(x, estimation, 'r-', label = 'Estimée')\n    plt.legend(loc='upper left', shadow=True, fontsize='x-large')\n\nestimation_3_components = gm_pdf(x, mu_em, sigma_em, alpha_em)    \nplot_distribution_comparison(estimation_3_components)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"fc9452fd-1982-4573-90cb-f5f9a210e25e","cell_type":"markdown","source":"## Comment\n\nOverall, the estimated distribution follows the trends of the empirical distribution. However, for values between -1 and 1, it seems that the estimation could be more accurate. In particular, two nearby local maxima appear to be distinct, but they are not differentiated by our initial estimator.","metadata":{}},{"id":"b3371e74-aef7-4808-8b3b-c0e63d0c97e3","cell_type":"code","source":"#  Initializing\n\nbatches = np.split(x_galaxies, [8,40,50])\nmu_0 = np.array([np.mean(batch) for batch in batches])\nsigma_0 = np.array([np.std(batch) for batch in batches])\nalpha_0 = np.array([0.2, 0.4, 0.3, 0.1])\n\nnb_iterations = 40\nresultat = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\nmu_em, sigma_em, alpha_em = resultat\n\nestimation_4_components = gm_pdf(x, mu_em, sigma_em, alpha_em)    \nplot_distribution_comparison(estimation_4_components)\nplt.show()\n\nprint_parameters(mu_em, sigma_em, alpha_em)","metadata":{},"outputs":[],"execution_count":null},{"id":"cf1e2c5d-38c7-4c7f-8c50-7180e11c927c","cell_type":"markdown","source":"## Comment\nWith a mixture of 4 Gaussians, the two local maxima in the middle are better represented, but the quality of the prediction for less probable velocities is reduced (particularly for large velocities, where a distinct peak was observed with 3 Gaussians).\n\nWe can attempt to improve this model by refining the clustering of the values. To do this, we will use the K-means method via sklearn.","metadata":{}},{"id":"de76813c-089f-4231-9866-ae06c3343336","cell_type":"code","source":"def kmeans_cluster(data, n_clusters):\n    kmeans = KMeans(n_clusters = n_clusters, random_state=0).fit(data.reshape(-1,1))\n    labeled_data = np.stack((kmeans.labels_, data), axis =1) # zip data with the label\n    labeled_data = labeled_data[labeled_data[:,0].argsort()] # order by label\n    return np.split(labeled_data[:,1], np.unique(labeled_data[:, 0], return_index = True)[1][1:]) # split values based on labels","metadata":{},"outputs":[],"execution_count":null},{"id":"7a83f2cc-da45-4a48-a6f2-5182aac9b824","cell_type":"code","source":"batches = kmeans_cluster(x_galaxies, 4)\nmu_0 = np.array([np.mean(batch) for batch in batches])\nsigma_0 = np.array([np.std(batch) for batch in batches])\nalpha_0 = np.array([0.2, 0.4, 0.3, 0.1])","metadata":{},"outputs":[],"execution_count":null},{"id":"2a89b3ec-eff2-4caa-81d1-2ef41add78d8","cell_type":"code","source":"nb_iterations = 40\nresultat = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\nmu_em, sigma_em, alpha_em = resultat\n\nestimation_4_components = gm_pdf(x, mu_em, sigma_em, alpha_em)    \nplot_distribution_comparison(estimation_4_components)\nplt.show()\n\nprint_parameters(mu_em, sigma_em, alpha_em)","metadata":{},"outputs":[],"execution_count":null},{"id":"79279b13-468b-45b6-a47f-60e3c65e70ef","cell_type":"markdown","source":"## Commentaire\nBy applying this clustering method, we observe that the estimation is closer to the empirical distribution in the \"less probable\" cases: we recover the peak around 2.75 and reduce the error for value sets with zero density. However, the two peaks around 0 are less well differentiated: the distribution is more similar to our case with 3 Gaussians, albeit more accurate.\n\nTherefore, we need a method to more intelligently choose the number of Gaussians. In the following, we attempt to use the BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion) to determine the optimal number of Gaussians.","metadata":{}},{"id":"7f6528bd-56bc-4ee4-b6c8-afe9941c461d","cell_type":"code","source":"n = 20    \nbics = []\naics = []\nfor i in range(1, n):\n    \n    gmm = GaussianMixture(n_components=i)\n    gmm.fit(x_galaxies.reshape(-1,1))\n    bics.append(gmm.bic(x_galaxies.reshape(-1,1)))\n    aics.append(gmm.aic(x_galaxies.reshape(-1,1)))\n    \n    \nplt.plot(np.arange(1,n),bics, label = 'BIC')\nplt.plot(np.arange(1,n),aics, label = 'AIC')\nplt.legend(loc='best')\nplt.show()\n\nprint(np.argmin(bics), np.argmin(aics))\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"0c28c7dc-8596-4885-adfe-f5198d65820c","cell_type":"markdown","source":"## Comment\n\nThe BIC and AIC criteria help favor parsimony, meaning a balance between the quality of the estimation and the number of model parameters. Based on the calculation above, we determine that the number of components in the mixture should be 2 according to the BIC criterion, and more than 10 according to the AIC criterion. The BIC penalizes the number of model parameters more than the AIC, which partially explains this difference.\n\nIt is important to note that, with each execution of the cell above, the minimum values of AIC and BIC can change (BIC typically takes values of 2 and 4, while AIC can vary between 12 and 18).\n\nNow, let's perform the estimation in both of these cases.","metadata":{}},{"id":"cf94ee44-c7f0-4883-9c6f-bc554065ff29","cell_type":"code","source":"def default_alpha_0(size):\n    return np.full((1,size), 1/size)[0]\n\n# Pour 2 gaussiennes\nbatches = kmeans_cluster(x_galaxies, 2)\nmu_0 = np.array([np.mean(batch) for batch in batches])\nsigma_0 = np.array([np.std(batch) for batch in batches])\nalpha_0 = default_alpha_0(2)\n\nmu_em, sigma_em, alpha_em = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n\nestimation = gm_pdf(x, mu_em, sigma_em, alpha_em)    \nplot_distribution_comparison(estimation)\nplt.show()\n\n# Pour 10 gaussiennes\nbatches = kmeans_cluster(x_galaxies, 10)\nmu_0 = np.array([np.mean(batch) for batch in batches])\nsigma_0 = np.array([np.std(batch) for batch in batches])\nalpha_0 = default_alpha_0(10)\n\nmu_em, sigma_em, alpha_em = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n\nestimation = gm_pdf(x, mu_em, sigma_em, alpha_em)    \nplot_distribution_comparison(estimation)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"fa5da483-7bde-4f31-a15d-83b580777f17","cell_type":"markdown","source":"## Comment\n\nFirst, we note that we limited the second estimation to 10 Gaussians, as beyond that, the results did not seem to display correctly.\n\nWe observe that by using the BIC, the estimated model is simple, capturing only the most distinctive variations in the data. In contrast, using 10 Gaussians leads to a very complex model that attempts to account for all apparent variations in the data.\n\nWe would likely prefer an intermediate number of Gaussians. Perhaps it would be beneficial to find another criterion more suited to the problem in order to determine the optimal number of Gaussians. Additionally, a criterion is needed to assess the effectiveness of the model without prior knowledge of the initial model.","metadata":{}},{"id":"5a804c92-a683-4238-83db-d911a07a595c","cell_type":"code","source":"# Pour 5 gaussiennes\nbatches = kmeans_cluster(x_galaxies, 5)\nmu_0 = np.array([np.mean(batch) for batch in batches])\nsigma_0 = np.array([np.std(batch) for batch in batches])\nalpha_0 = default_alpha_0(5)\n\nmu_em, sigma_em, alpha_em = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n\nestimation = gm_pdf(x, mu_em, sigma_em, alpha_em)    \nplot_distribution_comparison(estimation)\nplt.show()","metadata":{},"outputs":[],"execution_count":null}]}